{"cells":[{"cell_type":"code","execution_count":2,"id":"b5106e04-f9da-459f-a1cc-14e437fe001d","metadata":{"collapsed":false,"executionCancelledAt":null,"executionTime":21587,"jupyter":{"outputs_hidden":false,"source_hidden":false},"lastExecutedAt":1712212070715,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Import required libraries\nfrom pyspark.sql import SparkSession\nfrom pyspark.ml.feature import VectorAssembler\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.regression import RandomForestRegressor\nfrom pyspark.sql.functions import col, dayofmonth, month, year,  to_date, to_timestamp, weekofyear, dayofweek\nfrom pyspark.ml.feature import StringIndexer\nfrom pyspark.ml.evaluation import RegressionEvaluator\n\n# Initialize Spark session\nmy_spark = SparkSession.builder.appName(\"SalesForecast\").getOrCreate()\n\n# Importing sales data\nsales_data = my_spark.read.csv(\n    \"Online Retail.csv\", header=True, inferSchema=True, sep=\",\")\n\n# Convert InvoiceDate to datetime \nsales_data = sales_data.withColumn(\"InvoiceDate\", to_date(\n    to_timestamp(col(\"InvoiceDate\"), \"d/M/yyyy H:mm\")))\n\n# Aggregate data into daily intervals\ndaily_sales_data = sales_data.groupBy(\"Country\", \"StockCode\", \"InvoiceDate\", \"Year\", \"Month\", \"Day\", \"Week\", \"DayOfWeek\").agg({\"Quantity\": \"sum\",                                                                                                           \"UnitPrice\": \"avg\"})\n# Rename the target column\ndaily_sales_data = daily_sales_data.withColumnRenamed(\n    \"sum(Quantity)\", \"Quantity\")\n\n# Split the data into two sets based on the spliting date, \"2011-09-25\". All data up to and including this date should be in the training set, while data after this date should be in the testing set. Return a pandas Dataframe, pd_daily_train_data, containing, at least, the columns [\"Country\", \"StockCode\", \"InvoiceDate\", \"Quantity\"].\n\nsplit_date_train_test = \"2011-09-25\"\n\n# Creating the train and test datasets\ntrain_data = daily_sales_data.filter( col(\"InvoiceDate\") <= split_date_train_test)\ntest_data = daily_sales_data.filter(col(\"InvoiceDate\") > split_date_train_test)\n\npd_daily_train_data = train_data.toPandas()\n\n# Creating indexer for categorical columns\ncountry_indexer = StringIndexer(\n    inputCol=\"Country\", outputCol=\"CountryIndex\").setHandleInvalid(\"keep\")\nstock_code_indexer = StringIndexer(\n    inputCol=\"StockCode\", outputCol=\"StockCodeIndex\").setHandleInvalid(\"keep\")\n\n# Selectiong features columns\nfeature_cols = [\"CountryIndex\", \"StockCodeIndex\", \"Month\", \"Year\",\n                \"DayOfWeek\", \"Day\", \"Week\"]\n\n# Using vector assembler to combine features\nassembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n\n# Initializing a Random Forest model\nrf = RandomForestRegressor(\n    featuresCol=\"features\",\n    labelCol=\"Quantity\",\n    maxBins=4000\n)\n\n# Create a pipeline for staging the processes\npipeline = Pipeline(stages=[country_indexer, stock_code_indexer, assembler, rf])\n\n# Training the model\nmodel = pipeline.fit(train_data)\n\n# Getting test predictions\ntest_predictions = model.transform(test_data)\ntest_predictions = test_predictions.withColumn(\n    \"prediction\", col(\"prediction\").cast(\"double\"))\n\n# Provide the Mean Absolute Error (MAE) for your forecast? Return a double/floar \"mae\"\n\n# Initializing the evaluator\nmae_evaluator = RegressionEvaluator(\n    labelCol=\"Quantity\", predictionCol=\"prediction\", metricName=\"mae\")\n\n# Obtaining MAE\nmae = mae_evaluator.evaluate(test_predictions)\n\n# How many units will be sold during the  week 39 of 2011? Return an integer `quantity_sold_w39`.\n\n# Getting the weekly sales of all countries\nweekly_test_predictions = test_predictions.groupBy(\"Year\", \"Week\").agg({\"prediction\": \"sum\"})\n\n# Finding the quantity sold on the 39 week. \npromotion_week = weekly_test_predictions.filter(col('Week')==39)\n\n# Storing prediction as quantity_sold_w30\nquantity_sold_w39 = int(promotion_week.select(\"sum(prediction)\").collect()[0][0])\n\n# Stop the Spark session\nmy_spark.stop()\n","outputsMetadata":{"0":{"height":77,"type":"stream"},"1":{"height":57,"type":"stream"},"2":{"height":37,"type":"stream"},"3":{"height":97,"type":"stream"},"4":{"height":37,"type":"stream"},"5":{"height":57,"type":"stream"},"6":{"height":37,"type":"stream"}}},"outputs":[],"source":["# Import required libraries\n","from pyspark.sql import SparkSession\n","from pyspark.ml.feature import VectorAssembler\n","from pyspark.ml import Pipeline\n","from pyspark.ml.regression import RandomForestRegressor\n","from pyspark.sql.functions import col, dayofmonth, month, year,  to_date, to_timestamp, weekofyear, dayofweek\n","from pyspark.ml.feature import StringIndexer\n","from pyspark.ml.evaluation import RegressionEvaluator\n","\n","# Initialize Spark session\n","my_spark = SparkSession.builder.appName(\"SalesForecast\").getOrCreate()\n","\n","# Importing sales data\n","sales_data = my_spark.read.csv(\n","    \"Online Retail.csv\", header=True, inferSchema=True, sep=\",\")\n","\n","# Convert InvoiceDate to datetime \n","sales_data = sales_data.withColumn(\"InvoiceDate\", to_date(\n","    to_timestamp(col(\"InvoiceDate\"), \"d/M/yyyy H:mm\")))\n","\n","# Aggregate data into daily intervals\n","daily_sales_data = sales_data.groupBy(\"Country\", \"StockCode\", \"InvoiceDate\", \"Year\", \"Month\", \"Day\", \"Week\", \"DayOfWeek\").agg({\"Quantity\": \"sum\", \"UnitPrice\": \"avg\"})\n","# Rename the target column\n","daily_sales_data = daily_sales_data.withColumnRenamed(\n","    \"sum(Quantity)\", \"Quantity\")\n","\n","# Split the data into two sets based on the spliting date, \"2011-09-25\". All data up to and including this date should be in the training set, while data after this date should be in the testing set. Return a pandas Dataframe, pd_daily_train_data, containing, at least, the columns [\"Country\", \"StockCode\", \"InvoiceDate\", \"Quantity\"].\n","\n","split_date_train_test = \"2011-09-25\"\n","\n","# Creating the train and test datasets\n","train_data = daily_sales_data.filter( col(\"InvoiceDate\") <= split_date_train_test)\n","test_data = daily_sales_data.filter(col(\"InvoiceDate\") > split_date_train_test)\n","\n","pd_daily_train_data = train_data.toPandas()\n","\n","# Creating indexer for categorical columns\n","country_indexer = StringIndexer(\n","    inputCol=\"Country\", outputCol=\"CountryIndex\").setHandleInvalid(\"keep\")\n","stock_code_indexer = StringIndexer(\n","    inputCol=\"StockCode\", outputCol=\"StockCodeIndex\").setHandleInvalid(\"keep\")\n","\n","# Selectiong features columns\n","feature_cols = [\"CountryIndex\", \"StockCodeIndex\", \"Month\", \"Year\",\n","                \"DayOfWeek\", \"Day\", \"Week\"]\n","\n","# Using vector assembler to combine features\n","assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n","\n","# Initializing a Random Forest model\n","rf = RandomForestRegressor(\n","    featuresCol=\"features\",\n","    labelCol=\"Quantity\",\n","    maxBins=4000\n",")\n","\n","# Create a pipeline for staging the processes\n","pipeline = Pipeline(stages=[country_indexer, stock_code_indexer, assembler, rf])\n","\n","# Training the model\n","model = pipeline.fit(train_data)\n","\n","# Getting test predictions\n","test_predictions = model.transform(test_data)\n","test_predictions = test_predictions.withColumn(\n","    \"prediction\", col(\"prediction\").cast(\"double\"))\n","\n","# Provide the Mean Absolute Error (MAE) for your forecast? Return a double/floar \"mae\"\n","\n","# Initializing the evaluator\n","mae_evaluator = RegressionEvaluator(\n","    labelCol=\"Quantity\", predictionCol=\"prediction\", metricName=\"mae\")\n","\n","# Obtaining MAE\n","mae = mae_evaluator.evaluate(test_predictions)\n","\n","\n","# Getting the weekly sales of all countries\n","weekly_test_predictions = test_predictions.groupBy(\"Year\", \"Week\").agg({\"prediction\": \"sum\"})\n","\n","# Finding the quantity sold on the 39 week. \n","promotion_week = weekly_test_predictions.filter(col('Week')==39)\n","\n","# Storing prediction as quantity_sold_w30\n","quantity_sold_w39 = int(promotion_week.select(\"sum(prediction)\").collect()[0][0])\n","\n","# Stop the Spark session(ALWAYS)\n","my_spark.stop()\n"]},{"cell_type":"code","execution_count":3,"id":"38aa156d","metadata":{},"outputs":[{"data":{"text/plain":["88000"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["quantity_sold_w39 "]},{"cell_type":"code","execution_count":null,"id":"011b84aa","metadata":{},"outputs":[],"source":[]}],"metadata":{"colab":{"name":"Welcome to DataCamp Workspaces.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.4"}},"nbformat":4,"nbformat_minor":5}
